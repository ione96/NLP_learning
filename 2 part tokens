mport numpy as np
import pandas as pd
import re
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import TreebankWordTokenizer
from nltk.stem.porter import PorterStemmer

sentences = """Thomas Jefferson began building Monticello at the age of 26. \n"""
sentences += """Construction was done mostly by local masons and carpenters. \n"""
sentences += "He moved into the South Pavilion in 1770. \n"
sentences += """Turning Monticello into a neoclassical masterpiece was jefferson's obsession."""
corpus = {}

# sentences.split()
# token_sequence = str.split(sentences)
# vocab = sorted(set(token_sequence))
# ', '.join(vocab)
# num_tokens = len(token_sequence)
# vocab_size = len(vocab)
# onehot_vectors = np.zeros((num_tokens, vocab_size), int)
# for i, word in enumerate(token_sequence):
#    onehot_vectors[i, vocab.index(word)] = 1
# ' '.join(vocab)
# df = pd.DataFrame(pd.Series(dict([(token, 1) for token in
#                                 sentences.split()])), columns=['sent']).T

tokens = re.split(r'[-\s.,;!?]+', sentences) # разделение токенов по пробелам и 
print(tokens)                                # знакам препинания

tokenizer = RegexpTokenizer(r'\w+|$[0-9.]+|\S+') # токенизация regex
print(tokenizer.tokenize(sentences))

tokenizer = TreebankWordTokenizer()   # токенизация Treebank
print(tokenizer.tokenize(sentences))

tokenses = ['Houses', 'Visitors', 'CeNtEr']
normalized_tokens = [x.lower() for x in tokenses] # нормализация
print(normalized_tokens)

stemmer = PorterStemmer()  # стеммер
stem = ' '.join([stemmer.stem(w).strip("'") for w in "dish washer's washed dishes".split()])
print(stem)

for i, sent in enumerate(sentences.split('\n')):      # создание DataFrame
    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split())
df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T
print(df[df.columns[:10]])
